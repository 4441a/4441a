“””
Shannon Information Theory Toolkit
Based on “The Mathematical Theory of Communication” by Shannon & Weaver

This module implements the key mathematical concepts from information theory.
“””

import numpy as np
import matplotlib.pyplot as plt
from typing import List, Dict, Tuple, Union
from collections import Counter
import itertools

class InformationTheory:
“”“Core information theory calculations.”””

```
@staticmethod
def entropy(probabilities: List[float], base: int = 2) -> float:
    """
    Calculate Shannon entropy H = -∑ p_i log(p_i)
    
    Args:
        probabilities: List of probabilities (must sum to 1)
        base: Logarithm base (2 for bits, e for nats)
        
    Returns:
        Entropy in bits (or nats if base=e)
        
    Example:
        >>> InformationTheory.entropy([0.5, 0.5])  # Fair coin
        1.0
        >>> InformationTheory.entropy([0.9, 0.1])  # Biased coin
        0.469
    """
    probs = np.array(probabilities)
    # Remove zeros to avoid log(0)
    probs = probs[probs > 0]
    
    if base == 2:
        return -np.sum(probs * np.log2(probs))
    elif base == np.e:
        return -np.sum(probs * np.log(probs))
    else:
        return -np.sum(probs * np.log(probs) / np.log(base))

@staticmethod
def information_content(probability: float, base: int = 2) -> float:
    """
    Calculate information content I(p) = -log(p) = log(1/p)
    
    Args:
        probability: Probability of the event
        base: Logarithm base (2 for bits)
        
    Returns:
        Information content in bits
        
    Example:
        >>> InformationTheory.information_content(0.5)  # Fair coin
        1.0
        >>> InformationTheory.information_content(0.25)  # One outcome of fair die
        2.0
    """
    if probability <= 0 or probability > 1:
        raise ValueError("Probability must be in (0, 1]")
    
    if base == 2:
        return -np.log2(probability)
    elif base == np.e:
        return -np.log(probability)
    else:
        return -np.log(probability) / np.log(base)

@staticmethod
def joint_entropy(joint_probs: np.ndarray, base: int = 2) -> float:
    """
    Calculate joint entropy H(X,Y) = -∑∑ p(x_i, y_j) log p(x_i, y_j)
    
    Args:
        joint_probs: 2D array of joint probabilities
        base: Logarithm base
        
    Returns:
        Joint entropy in bits
        
    Example:
        >>> # Two independent fair coins
        >>> probs = np.array([[0.25, 0.25], [0.25, 0.25]])
        >>> InformationTheory.joint_entropy(probs)
        2.0
    """
    probs = joint_probs[joint_probs > 0]
    if base == 2:
        return -np.sum(probs * np.log2(probs))
    else:
        return -np.sum(probs * np.log(probs) / np.log(base))

@staticmethod
def conditional_entropy(joint_probs: np.ndarray, base: int = 2) -> Tuple[float, float]:
    """
    Calculate conditional entropies H(Y|X) and H(X|Y)
    Using: H(Y|X) = H(X,Y) - H(X)
    
    Args:
        joint_probs: 2D array of joint probabilities
        base: Logarithm base
        
    Returns:
        Tuple of (H(Y|X), H(X|Y))
        
    Example:
        >>> # Independent coins
        >>> probs = np.array([[0.25, 0.25], [0.25, 0.25]])
        >>> InformationTheory.conditional_entropy(probs)
        (1.0, 1.0)  # Knowing one gives no info about the other
    """
    h_xy = InformationTheory.joint_entropy(joint_probs, base)
    
    # Marginal probabilities
    p_x = joint_probs.sum(axis=1)  # Sum over columns
    p_y = joint_probs.sum(axis=0)  # Sum over rows
    
    h_x = InformationTheory.entropy(p_x, base)
    h_y = InformationTheory.entropy(p_y, base)
    
    h_y_given_x = h_xy - h_x
    h_x_given_y = h_xy - h_y
    
    return h_y_given_x, h_x_given_y

@staticmethod
def mutual_information(joint_probs: np.ndarray, base: int = 2) -> float:
    """
    Calculate mutual information I(X;Y) = H(X) + H(Y) - H(X,Y)
    
    This measures how much information X and Y share.
    
    Args:
        joint_probs: 2D array of joint probabilities
        base: Logarithm base
        
    Returns:
        Mutual information in bits
        
    Example:
        >>> # Identical variables (Y = X)
        >>> probs = np.array([[0.5, 0], [0, 0.5]])
        >>> InformationTheory.mutual_information(probs)
        1.0  # They share all their information
    """
    h_xy = InformationTheory.joint_entropy(joint_probs, base)
    
    p_x = joint_probs.sum(axis=1)
    p_y = joint_probs.sum(axis=0)
    
    h_x = InformationTheory.entropy(p_x, base)
    h_y = InformationTheory.entropy(p_y, base)
    
    return h_x + h_y - h_xy

@staticmethod
def max_entropy(n_symbols: int, base: int = 2) -> float:
    """
    Calculate maximum possible entropy for n symbols.
    Maximum occurs when all symbols are equally likely.
    
    H_max = log(n)
    
    Args:
        n_symbols: Number of symbols in alphabet
        base: Logarithm base
        
    Returns:
        Maximum entropy in bits
    """
    if base == 2:
        return np.log2(n_symbols)
    else:
        return np.log(n_symbols) / np.log(base)

@staticmethod
def redundancy(probabilities: List[float]) -> float:
    """
    Calculate redundancy = (H_max - H) / H_max
    
    Shannon's measure of how much "extra" structure exists
    beyond random selection.
    
    Args:
        probabilities: List of symbol probabilities
        
    Returns:
        Redundancy as a fraction (0 to 1)
        
    Example:
        >>> # English has ~73% redundancy
        >>> InformationTheory.redundancy([0.5, 0.25, 0.125, 0.125])
        0.125  # 12.5% redundancy
    """
    h = InformationTheory.entropy(probabilities)
    h_max = InformationTheory.max_entropy(len(probabilities))
    return (h_max - h) / h_max
```

class DiscreteChannel:
“”“Models for discrete communication channels.”””

```
def __init__(self, transition_matrix: np.ndarray):
    """
    Initialize a discrete channel with transition probabilities.
    
    Args:
        transition_matrix: Matrix where element [i,j] = P(output=j | input=i)
                         Rows must sum to 1.
    """
    self.transition_matrix = np.array(transition_matrix)
    if not np.allclose(self.transition_matrix.sum(axis=1), 1.0):
        raise ValueError("Each row of transition matrix must sum to 1")
    
    self.n_inputs = transition_matrix.shape[0]
    self.n_outputs = transition_matrix.shape[1]

@classmethod
def binary_symmetric_channel(cls, error_prob: float):
    """
    Create a Binary Symmetric Channel (BSC).
    
    This is Shannon's canonical example:
    - Input: {0, 1}
    - Output: {0, 1}
    - Each bit flips with probability p
    
    Args:
        error_prob: Probability of bit flip (p)
        
    Returns:
        DiscreteChannel instance
    """
    p = error_prob
    transition = np.array([
        [1-p, p],    # If send 0: receive 0 with prob 1-p, receive 1 with prob p
        [p, 1-p]     # If send 1: receive 1 with prob 1-p, receive 0 with prob p
    ])
    return cls(transition)

def capacity(self) -> float:
    """
    Calculate channel capacity C = max[I(X;Y)]
    
    This is maximized over all possible input distributions.
    Uses numerical optimization for general channels.
    
    Returns:
        Channel capacity in bits per transmission
    """
    from scipy.optimize import minimize
    
    def negative_mutual_info(input_probs):
        """Function to minimize (negative of mutual information)."""
        # Ensure probabilities sum to 1
        input_probs = np.append(input_probs, 1 - input_probs.sum())
        if input_probs[-1] < 0:
            return 1e10  # Penalty for invalid probabilities
        
        # Calculate joint probabilities
        joint = np.outer(input_probs, np.ones(self.n_outputs))
        joint *= self.transition_matrix
        
        # Calculate mutual information
        mi = InformationTheory.mutual_information(joint)
        return -mi  # Negative because we're minimizing
    
    # Initial guess: uniform distribution
    x0 = np.ones(self.n_inputs - 1) / self.n_inputs
    
    # Optimize
    result = minimize(
        negative_mutual_info,
        x0,
        method='SLSQP',
        bounds=[(0, 1)] * (self.n_inputs - 1)
    )
    
    return -result.fun

def bsc_capacity(self, error_prob: float) -> float:
    """
    Calculate capacity of Binary Symmetric Channel analytically.
    
    C = 1 - H(p) where H(p) is binary entropy function
    
    Args:
        error_prob: Error probability p
        
    Returns:
        Capacity in bits per transmission
    """
    if error_prob == 0 or error_prob == 1:
        return 1.0
    h_p = InformationTheory.entropy([error_prob, 1 - error_prob])
    return 1 - h_p
```

class ContinuousChannel:
“”“Models for continuous (analog) channels.”””

```
@staticmethod
def shannon_hartley_capacity(bandwidth: float, snr: float) -> float:
    """
    Calculate capacity of Gaussian channel (Shannon-Hartley theorem).
    
    C = B * log₂(1 + SNR)
    
    Args:
        bandwidth: Bandwidth in Hz
        snr: Signal-to-noise ratio (S/N, not in dB)
        
    Returns:
        Capacity in bits per second
        
    Example:
        >>> # Telephone line: 3000 Hz, SNR = 1000
        >>> ContinuousChannel.shannon_hartley_capacity(3000, 1000)
        29903.5  # ~30 kbps
    """
    return bandwidth * np.log2(1 + snr)

@staticmethod
def snr_from_db(snr_db: float) -> float:
    """Convert SNR from decibels to linear scale."""
    return 10 ** (snr_db / 10)

@staticmethod
def snr_to_db(snr_linear: float) -> float:
    """Convert SNR from linear scale to decibels."""
    return 10 * np.log10(snr_linear)

@staticmethod
def gaussian_entropy(variance: float) -> float:
    """
    Calculate differential entropy of Gaussian distribution.
    
    h(X) = (1/2) * log₂(2πeσ²)
    
    Args:
        variance: Variance σ² of the Gaussian
        
    Returns:
        Differential entropy in bits
    """
    return 0.5 * np.log2(2 * np.pi * np.e * variance)
```

class SourceCoding:
“”“Source coding (compression) algorithms.”””

```
@staticmethod
def huffman_code(probabilities: Dict[str, float]) -> Dict[str, str]:
    """
    Generate optimal Huffman code for given symbol probabilities.
    
    This is the optimal prefix-free code that achieves average length
    closest to entropy.
    
    Args:
        probabilities: Dictionary mapping symbols to probabilities
        
    Returns:
        Dictionary mapping symbols to binary codewords
        
    Example:
        >>> probs = {'A': 0.5, 'B': 0.25, 'C': 0.125, 'D': 0.125}
        >>> code = SourceCoding.huffman_code(probs)
        >>> code
        {'A': '0', 'B': '10', 'C': '110', 'D': '111'}
    """
    import heapq
    
    # Create a min-heap of (probability, unique_id, symbol/subtree)
    heap = [[prob, i, sym] for i, (sym, prob) in enumerate(probabilities.items())]
    heapq.heapify(heap)
    
    counter = len(heap)
    
    # Build Huffman tree
    while len(heap) > 1:
        lo = heapq.heappop(heap)
        hi = heapq.heappop(heap)
        
        # Create internal node
        node = [lo[0] + hi[0], counter, {0: lo, 1: hi}]
        heapq.heappush(heap, node)
        counter += 1
    
    # Extract codes by traversing tree
    root = heap[0]
    codes = {}
    
    def traverse(node, code=''):
        if isinstance(node[2], str):  # Leaf node
            codes[node[2]] = code if code else '0'
        else:  # Internal node
            traverse(node[2][0], code + '0')
            traverse(node[2][1], code + '1')
    
    traverse(root)
    return codes

@staticmethod
def average_code_length(probabilities: Dict[str, float], 
                       code: Dict[str, str]) -> float:
    """
    Calculate average codeword length L = ∑ p_i * l_i
    
    Args:
        probabilities: Symbol probabilities
        code: Dictionary mapping symbols to codewords
        
    Returns:
        Average bits per symbol
    """
    return sum(probabilities[sym] * len(code[sym]) 
              for sym in probabilities)

@staticmethod
def coding_efficiency(probabilities: Dict[str, float],
                     code: Dict[str, str]) -> float:
    """
    Calculate efficiency = H / L
    
    Where H is entropy and L is average code length.
    Perfect efficiency = 1.0 (achieving entropy bound)
    
    Args:
        probabilities: Symbol probabilities
        code: Dictionary mapping symbols to codewords
        
    Returns:
        Efficiency (0 to 1)
    """
    probs_list = list(probabilities.values())
    h = InformationTheory.entropy(probs_list)
    l = SourceCoding.average_code_length(probabilities, code)
    return h / l if l > 0 else 0
```

class TextAnalysis:
“”“Analyze entropy and redundancy of text.”””

```
@staticmethod
def letter_frequencies(text: str, normalize: bool = True) -> Dict[str, float]:
    """
    Calculate letter frequencies in text.
    
    Args:
        text: Input text
        normalize: If True, return probabilities; if False, return counts
        
    Returns:
        Dictionary of letter frequencies/probabilities
    """
    text = text.upper()
    text = ''.join(c for c in text if c.isalpha() or c.isspace())
    
    counter = Counter(text)
    
    if normalize:
        total = sum(counter.values())
        return {k: v/total for k, v in counter.items()}
    return dict(counter)

@staticmethod
def ngram_frequencies(text: str, n: int = 2, 
                     normalize: bool = True) -> Dict[str, float]:
    """
    Calculate n-gram frequencies.
    
    Args:
        text: Input text
        n: Length of n-grams (2=bigrams, 3=trigrams, etc.)
        normalize: If True, return probabilities
        
    Returns:
        Dictionary of n-gram frequencies
    """
    text = text.upper()
    text = ''.join(c for c in text if c.isalpha() or c.isspace())
    
    ngrams = [''.join(text[i:i+n]) for i in range(len(text)-n+1)]
    counter = Counter(ngrams)
    
    if normalize:
        total = sum(counter.values())
        return {k: v/total for k, v in counter.items()}
    return dict(counter)

@staticmethod
def text_entropy(text: str, n: int = 1) -> float:
    """
    Calculate n-gram entropy of text.
    
    Args:
        text: Input text
        n: Order (1=unigram, 2=bigram, etc.)
        
    Returns:
        Entropy in bits per n-gram
    """
    freqs = TextAnalysis.ngram_frequencies(text, n, normalize=True)
    probs = list(freqs.values())
    h = InformationTheory.entropy(probs)
    return h / n  # Per symbol
```

class Visualization:
“”“Plotting functions for information theory concepts.”””

```
@staticmethod
def plot_binary_entropy():
    """Plot binary entropy function H(p) vs p."""
    p = np.linspace(0.001, 0.999, 1000)
    h = np.array([InformationTheory.entropy([pi, 1-pi]) for pi in p])
    
    plt.figure(figsize=(10, 6))
    plt.plot(p, h, 'b-', linewidth=2)
    plt.xlabel('Probability p', fontsize=12)
    plt.ylabel('Entropy H(p) [bits]', fontsize=12)
    plt.title('Binary Entropy Function', fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.axhline(y=1, color='r', linestyle='--', alpha=0.5, label='Maximum (1 bit)')
    plt.legend()
    plt.tight_layout()
    plt.show()

@staticmethod
def plot_bsc_capacity():
    """Plot BSC capacity vs error probability."""
    p = np.linspace(0, 0.5, 1000)
    c = np.array([1 - InformationTheory.entropy([pi, 1-pi]) if pi < 0.5 else 0 
                 for pi in p])
    
    plt.figure(figsize=(10, 6))
    plt.plot(p, c, 'b-', linewidth=2)
    plt.xlabel('Error Probability p', fontsize=12)
    plt.ylabel('Capacity C [bits/transmission]', fontsize=12)
    plt.title('Binary Symmetric Channel Capacity', fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.axhline(y=0, color='r', linestyle='--', alpha=0.5)
    plt.axvline(x=0.5, color='r', linestyle='--', alpha=0.5, 
               label='p=0.5: Useless channel')
    plt.legend()
    plt.tight_layout()
    plt.show()

@staticmethod
def plot_shannon_hartley(bandwidth: float = 4000):
    """
    Plot Shannon-Hartley capacity vs SNR.
    
    Args:
        bandwidth: Channel bandwidth in Hz
    """
    snr_db = np.linspace(-10, 40, 1000)
    snr_linear = 10 ** (snr_db / 10)
    capacity = bandwidth * np.log2(1 + snr_linear)
    
    plt.figure(figsize=(10, 6))
    plt.plot(snr_db, capacity / 1000, 'b-', linewidth=2)
    plt.xlabel('SNR [dB]', fontsize=12)
    plt.ylabel('Capacity [kbps]', fontsize=12)
    plt.title(f'Shannon-Hartley Capacity (Bandwidth = {bandwidth} Hz)', 
             fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
```

# ============================================================================

# EXAMPLE USAGE AND DEMONSTRATIONS

# ============================================================================

def demo_basic_entropy():
“”“Demonstrate basic entropy calculations.”””
print(”=”*70)
print(“BASIC ENTROPY CALCULATIONS”)
print(”=”*70)

```
# Fair coin
print("\n1. Fair Coin:")
h = InformationTheory.entropy([0.5, 0.5])
print(f"   P(H) = 0.5, P(T) = 0.5")
print(f"   Entropy = {h:.3f} bits")

# Biased coin
print("\n2. Biased Coin:")
h = InformationTheory.entropy([0.9, 0.1])
print(f"   P(H) = 0.9, P(T) = 0.1")
print(f"   Entropy = {h:.3f} bits")

# Fair die
print("\n3. Fair Die:")
h = InformationTheory.entropy([1/6]*6)
print(f"   Six equally likely outcomes")
print(f"   Entropy = {h:.3f} bits")

# Maximum entropy
print("\n4. Maximum Entropy:")
h_max = InformationTheory.max_entropy(6)
print(f"   Maximum possible for 6 symbols = {h_max:.3f} bits")
```

def demo_channel_capacity():
“”“Demonstrate channel capacity calculations.”””
print(”\n” + “=”*70)
print(“CHANNEL CAPACITY”)
print(”=”*70)

```
# BSC with different error rates
print("\n1. Binary Symmetric Channel:")
for p in [0.0, 0.01, 0.1, 0.5]:
    channel = DiscreteChannel.binary_symmetric_channel(p)
    c = channel.bsc_capacity(p)
    print(f"   Error probability p = {p:.2f}: Capacity = {c:.4f} bits")

# Shannon-Hartley
print("\n2. Gaussian Channel (Shannon-Hartley):")
print(f"   Telephone line (B=3000 Hz, SNR=30 dB):")
snr = ContinuousChannel.snr_from_db(30)
c = ContinuousChannel.shannon_hartley_capacity(3000, snr)
print(f"   Capacity = {c:.1f} bits/second")
```

def demo_huffman_coding():
“”“Demonstrate Huffman coding.”””
print(”\n” + “=”*70)
print(“HUFFMAN CODING”)
print(”=”*70)

```
# Example from Shannon's paper
probs = {'A': 0.5, 'B': 0.25, 'C': 0.125, 'D': 0.125}

print("\nSymbol probabilities:")
for sym, prob in probs.items():
    print(f"   {sym}: {prob}")

# Calculate entropy
h = InformationTheory.entropy(list(probs.values()))
print(f"\nEntropy H = {h:.3f} bits per symbol")

# Generate Huffman code
code = SourceCoding.huffman_code(probs)
print("\nHuffman code:")
for sym in sorted(code.keys()):
    print(f"   {sym}: {code[sym]}")

# Calculate average length
avg_len = SourceCoding.average_code_length(probs, code)
print(f"\nAverage code length L = {avg_len:.3f} bits per symbol")

efficiency = SourceCoding.coding_efficiency(probs, code)
print(f"Efficiency η = H/L = {efficiency:.3f} (100% = optimal)")
```

def demo_text_analysis():
“”“Demonstrate text analysis.”””
print(”\n” + “=”*70)
print(“TEXT ANALYSIS”)
print(”=”*70)

```
sample_text = """
The quick brown fox jumps over the lazy dog. 
This is a sample text for demonstrating Shannon's information theory.
"""

# Letter frequencies
print("\nTop 10 letter frequencies:")
freqs = TextAnalysis.letter_frequencies(sample_text)
sorted_freqs = sorted(freqs.items(), key=lambda x: x[1], reverse=True)[:10]
for letter, freq in sorted_freqs:
    if letter != ' ':
        print(f"   {letter}: {freq:.4f}")

# Entropy calculations
print("\nEntropy analysis:")
h1 = TextAnalysis.text_entropy(sample_text, n=1)
print(f"   Unigram entropy: {h1:.3f} bits/letter")

h2 = TextAnalysis.text_entropy(sample_text, n=2)
print(f"   Bigram entropy: {h2:.3f} bits/letter")

# Redundancy
h_max = InformationTheory.max_entropy(27)  # 26 letters + space
redundancy = (h_max - h1) / h_max
print(f"   Redundancy: {redundancy:.1%}")
print(f"   (English typically has ~70-75% redundancy)")
```

if **name** == “**main**”:
print(”\n” + “=”*70)
print(“SHANNON’S INFORMATION THEORY - DEMONSTRATION”)
print(”=”*70)

```
# Run all demonstrations
demo_basic_entropy()
demo_channel_capacity()
demo_huffman_coding()
demo_text_analysis()

print("\n" + "="*70)
print("To create visualizations, run:")
print("  Visualization.plot_binary_entropy()")
print("  Visualization.plot_bsc_capacity()")
print("  Visualization.plot_shannon_hartley()")
print("="*70 + "\n")
```